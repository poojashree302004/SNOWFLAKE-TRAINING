# ‚ùÑÔ∏è Snowflake Staging ‚Äì Complete Tutorial

## üß≠ What is Staging in Snowflake?

In **Snowflake**, a **stage** is a temporary or permanent location where data files are stored **before being loaded into a Snowflake table** or **after being unloaded from it**.

Think of it as a **landing zone** between your data source (like Azure Blob, AWS S3, or local files) and Snowflake tables.

---

## üß© Types of Stages

| Type | Description | Example |
|------|--------------|----------|
| **User Stage** | Automatically created for each user | `@~` |
| **Table Stage** | Automatically created for each table | `@%MY_TABLE` |
| **Named Internal Stage** | Manually created internal storage inside Snowflake | `@my_internal_stage` |
| **External Stage** | References cloud storage (Azure Blob, AWS S3, GCP) | `@my_azure_stage` |

---

## üí° Why Do We Use Staging in Snowflake?

| Purpose | Explanation |
|----------|--------------|
| **Data Loading** | Upload raw files (CSV, JSON, Parquet) into a stage first, then load them into tables using `COPY INTO`. |
| **Data Unloading** | Export Snowflake table data into staged files for sharing or backup. |
| **Intermediate Storage** | Safe area to inspect or validate files before loading. |
| **Integration Point** | Enables integration with cloud storage systems (Azure Blob, AWS S3, etc.) for pipelines. |

**Analogy:**  
> Think of staging as a ‚Äúwaiting room‚Äù where your data stays before entering the main database.

---

## ‚è∞ When to Use Staging

| Scenario | When to Use |
|-----------|-------------|
| **Bulk Data Loading** | When your data resides as CSV/JSON/Parquet files locally or in the cloud. |
| **Data Validation** | When you want to verify or clean files before inserting. |
| **Data Exporting** | To back up or share table data. |
| **External Integration** | To connect Snowflake with Azure Data Lake, AWS S3, or GCP buckets. |

---

## üè¢ Real-World Use Case: Loading Data from Azure Blob Storage

### üéØ Scenario

A retail company stores daily sales data (`sales_2025_10_13.csv`) in **Azure Blob Storage**.  
We need to **load this data into Snowflake** for reporting and analysis.

---

## ‚öôÔ∏è Step-by-Step Implementation

### üß© Step 1 ‚Äì Create File Format

```sql
CREATE OR REPLACE FILE FORMAT my_csv_format
  TYPE = 'CSV'
  FIELD_OPTIONALLY_ENCLOSED_BY = '"'
  SKIP_HEADER = 1;
```

‚úÖ *Tells Snowflake that files are CSVs, enclosed in quotes, with headers.*

---

### ‚òÅÔ∏è Step 2 ‚Äì Create External Stage

```sql
CREATE OR REPLACE STAGE azure_stage
  URL='azure://myblobaccount.blob.core.windows.net/salesdata'
  CREDENTIALS=(AZURE_SAS_TOKEN='?sv=2024-08-...')
  FILE_FORMAT = my_csv_format;
```

‚úÖ *Creates a stage `azure_stage` pointing to your Azure container.*

---

### üßæ Step 3 ‚Äì Verify Files in Stage

```sql
LIST @azure_stage;
```

**Output Example:**
```
+-------------------------------------------+
| name                                      |
|-------------------------------------------|
| salesdata/sales_2025_10_13.csv            |
+-------------------------------------------+
```

---

### üì• Step 4 ‚Äì Create Target Table in Snowflake

```sql
CREATE OR REPLACE TABLE SALES (
    ID INT,
    CUSTOMER_NAME STRING,
    REGION STRING,
    SALES_AMOUNT FLOAT,
    SALES_DATE DATE
);
```

---

### üöÄ Step 5 ‚Äì Load Data from Stage into Table

```sql
COPY INTO SALES
FROM @azure_stage/sales_2025_10_13.csv
FILE_FORMAT = (FORMAT_NAME = my_csv_format)
ON_ERROR = 'CONTINUE';
```

‚úÖ *Loads all records from the staged CSV file into the SALES table.*

---

### üîç Step 6 ‚Äì Verify Loaded Data

```sql
SELECT * FROM SALES LIMIT 10;
```

**Example Output:**
```
| ID | CUSTOMER_NAME | REGION | SALES_AMOUNT | SALES_DATE |
|----|----------------|--------|---------------|------------|
| 1  | John Doe       | West   | 5000.00       | 2025-10-13 |
| 2  | Priya Sharma   | East   | 3200.50       | 2025-10-13 |
```

---

### üßπ Step 7 ‚Äì Unload Data Back to Stage (Optional)

```sql
COPY INTO @azure_stage/exported_sales/
FROM SALES
FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='"');
```

‚úÖ *Exports table data back to Azure Blob as CSV files.*

---

## üìò Summary

| Concept | Description | Command |
|----------|--------------|----------|
| **Stage Creation** | Create link to internal or external storage | `CREATE STAGE` |
| **List Files** | Verify files in stage | `LIST @stage_name` |
| **Load Data** | Copy from stage to table | `COPY INTO <table>` |
| **Unload Data** | Export from table to stage | `COPY INTO @stage_name` |

---

## üíº Business Use Cases

| Industry | Example |
|-----------|----------|
| **Retail** | Load daily POS transaction CSVs from Azure Blob into Snowflake. |
| **Finance** | Ingest daily transaction files or stock feeds securely from S3. |
| **Healthcare** | Stage patient data files before ETL to ensure schema validation. |
| **IoT / Manufacturing** | Stage sensor readings before transforming into analytics tables. |

---

## üß† Trainer Notes

- ‚úÖ Staging = Waiting Room Analogy helps learners visualize the concept.  
- ‚úÖ Use staging to decouple ingestion and transformation.  
- ‚úÖ Demonstrate both internal and external stages in labs.

---

**Author:** *Prepared by Snowflake Specialist Trainer (GPT-5)*  
**Audience:** Beginner to Intermediate Data Engineers  
**Session:** Day 1 ‚Äì Snowflake & Azure Integration
