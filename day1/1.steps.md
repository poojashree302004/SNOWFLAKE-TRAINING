# Lab 3 â€“ Connect Azure Databricks to Snowflake

## ğŸ¯ Objective  
Read Snowflake data directly inside **Azure Databricks** using the **Snowflake Spark Connector**.

---

## ğŸªœ Step-by-Step Instructions

### Step 1 â€“ Open Azure Databricks
1. Go to [https://portal.azure.com](https://portal.azure.com)
2. Open your **Azure Databricks workspace**
3. Click **Launch Workspace** â€” this opens the **Databricks UI**

---
![alt text](image.png)
![alt text](image-1.png)

![alt text](image-2.png)

# ğŸš€ Lab â€“ Create a Cluster (Now Called â€œComputeâ€) in Azure Databricks

## ğŸ¯ Objective
Learn how to create a **Compute (formerly Cluster)** in Azure Databricks, configure it properly for **Snowflake integration**, and attach a **notebook** to run your code.

---

## âš™ï¸ Step 1 â€” Create Compute

1. On the **top-right corner**, click the **blue button â†’ â€œCreate computeâ€**  
   *(This replaces â€œCreate Clusterâ€ in the older UI.)*
2. Click **Create compute** to start the setup.

---

## âš™ï¸ Step 2 â€” Fill in Compute Configuration

Youâ€™ll see a configuration form similar to the one below:

| **Field** | **What to Enter** | **Example / Notes** |
|------------|------------------|---------------------|
| **Compute name** | A simple name for your cluster | `hexa-cluster` |
| **Cluster mode** | Keep default *(Single Node)* if youâ€™re testing | âœ… Ideal for small labs |
| **Databricks runtime version** | Choose a version with **Spark 3.3** | `11.3 LTS (Scala 2.12, Spark 3.3.0)`<br>âœ… Required for **Snowflake connector** |
| **Node type** | Default or choose `Standard_DS3_v2` | Recommended for balanced cost |
| **Min workers / Max workers** | Keep `1` and `1` for lab | Keeps cluster minimal |
| **Terminate after** | 120 minutes (or less) | â±ï¸ Saves cost when idle |
| **Access mode** | Choose **Single user** or **No isolation shared** | Recommended for training/testing |

Once done, scroll to the bottom and click **Create compute**.
![alt text](image-3.png)
![alt text](image-4.png)
---

## â³ Step 3 â€” Wait for the Cluster to Start

- After creation, your new compute will appear in the list with status:
  - **Pending â†’ Running**
- Wait until you see a **green dot** indicating the cluster is **active**.

âœ… Your compute environment is now ready.

---
# âš™ï¸ Step 3.1 â€“ Install the Snowflake Spark Connector

Before running your Databricks notebook, you must install the **Snowflake Spark Connector** on your compute (cluster).

---

## ğŸ§© Option A â€” Easiest (UI Method)

1. Go to the **Compute** page in Databricks.
2. Click on your cluster name (for example, **`hexa-cluster`**).
3. Inside the cluster page, open the **Libraries** tab.
4. Click **Install New â†’ Maven**.
5. ![alt text](image-7.png)
6. Paste the following **Maven coordinate**:

   ```text
   com.snowflake:spark-snowflake_2.12:2.11.0-spark_3.3
   or 
   net.snowflake:spark-snowflake_2.12:2.12.0-spark_3.4

   

   ```
   1. Click Install.
   2. Wait until you see the message:
âœ… â€œInstalled Successfullyâ€


![alt text](image-9.png)
## ğŸ’¡Notes

For Spark 3.5 runtime, the above version still works fine.
Alternatively, you can use:
```text
com.snowflake:spark-snowflake_2.12:2.12.0-spark_3.5
```
![alt text](image-8.png)
Once installed, the connector allows Databricks to communicate with Snowflake seamlessly via JDBC and Spark APIs.

## ğŸš€ After Installation

Restart your cluster (or detach + re-attach it).

Re-run your notebook cell:

df = spark.read.format("snowflake").options(**options).option("dbtable", "SALES").load()
df.show()


You should now see the Snowflake data appear successfully.
## ğŸ§  Step 4 â€” Attach Notebook

Once your compute (cluster) is running:
![alt text](image-5.png)
1. Navigate to **Workspace â†’ Your Folder â†’ Create â†’ Notebook**  
2. Name it something like: `Snowflake_Connection`
3. Choose **Language: Python**
4. At the **top-left**, select your **running compute** from the dropdown menu to attach it.
5. Paste your **Lab 3 Snowflake connection code** into this notebook.

---
![alt text](image-6.png)

```python
options = {
    "sfURL": "YGVSEGU-JE02852.snowflakecomputing.com",
    "sfDatabase": "TRAINING_DB",
    "sfSchema": "RAW",
    "sfWarehouse": "COMPUTE_WH",
    "sfRole": "SYSADMIN",
    "sfUser": "geetha",
    "sfPassword": "MySnowflakecred1"
}
df = spark.read.format("snowflake").options(**options).option("dbtable", "SALES").load()
df.show()
```
![alt text](image-10.png)
## ğŸ’¡ Summary

| **Step** | **Action** | **Where** |
|-----------|-------------|------------|
| 1 | Click **â€œCreate computeâ€** | Top-right of Compute screen |
| 2 | Fill in Spark runtime, name, and size | Popup configuration form |
| 3 | Click **â€œCreate computeâ€** | Bottom of the form |
| 4 | Wait for status **Running** | Compute list |
| 5 | Create **Notebook** and **Attach compute** | Workspace â†’ Notebook |

---

## âœ… Next Steps
Once the compute is ready and your notebook is attached:
- Test your **Snowflake connection code** (Lab 3).
- Run sample queries to validate Sparkâ€“Snowflake integration.
- Monitor cluster logs from the **Compute tab** for debugging or performance review.
