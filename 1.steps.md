# Lab 3 – Connect Azure Databricks to Snowflake

## 🎯 Objective  
Read Snowflake data directly inside **Azure Databricks** using the **Snowflake Spark Connector**.

---

## 🪜 Step-by-Step Instructions

### Step 1 – Open Azure Databricks
1. Go to [https://portal.azure.com](https://portal.azure.com)
2. Open your **Azure Databricks workspace**
3. Click **Launch Workspace** — this opens the **Databricks UI**

---
![alt text](image.png)
![alt text](image-1.png)

![alt text](image-2.png)

# 🚀 Lab – Create a Cluster (Now Called “Compute”) in Azure Databricks

## 🎯 Objective
Learn how to create a **Compute (formerly Cluster)** in Azure Databricks, configure it properly for **Snowflake integration**, and attach a **notebook** to run your code.

---

## ⚙️ Step 1 — Create Compute

1. On the **top-right corner**, click the **blue button → “Create compute”**  
   *(This replaces “Create Cluster” in the older UI.)*
2. Click **Create compute** to start the setup.

---

## ⚙️ Step 2 — Fill in Compute Configuration

You’ll see a configuration form similar to the one below:

| **Field** | **What to Enter** | **Example / Notes** |
|------------|------------------|---------------------|
| **Compute name** | A simple name for your cluster | `hexa-cluster` |
| **Cluster mode** | Keep default *(Single Node)* if you’re testing | ✅ Ideal for small labs |
| **Databricks runtime version** | Choose a version with **Spark 3.3** | `11.3 LTS (Scala 2.12, Spark 3.3.0)`<br>✅ Required for **Snowflake connector** |
| **Node type** | Default or choose `Standard_DS3_v2` | Recommended for balanced cost |
| **Min workers / Max workers** | Keep `1` and `1` for lab | Keeps cluster minimal |
| **Terminate after** | 120 minutes (or less) | ⏱️ Saves cost when idle |
| **Access mode** | Choose **Single user** or **No isolation shared** | Recommended for training/testing |

Once done, scroll to the bottom and click **Create compute**.
![alt text](image-3.png)
![alt text](image-4.png)
---

## ⏳ Step 3 — Wait for the Cluster to Start

- After creation, your new compute will appear in the list with status:
  - **Pending → Running**
- Wait until you see a **green dot** indicating the cluster is **active**.

✅ Your compute environment is now ready.

---
# ⚙️ Step 3.1 – Install the Snowflake Spark Connector

Before running your Databricks notebook, you must install the **Snowflake Spark Connector** on your compute (cluster).

---

## 🧩 Option A — Easiest (UI Method)

1. Go to the **Compute** page in Databricks.
2. Click on your cluster name (for example, **`hexa-cluster`**).
3. Inside the cluster page, open the **Libraries** tab.
4. Click **Install New → Maven**.
5. ![alt text](image-7.png)
6. Paste the following **Maven coordinate**:

   ```text
   com.snowflake:spark-snowflake_2.12:2.11.0-spark_3.3
   or 
   net.snowflake:spark-snowflake_2.12:2.12.0-spark_3.4

   

   ```
   1. Click Install.
   2. Wait until you see the message:
✅ “Installed Successfully”


![alt text](image-9.png)
## 💡Notes

For Spark 3.5 runtime, the above version still works fine.
Alternatively, you can use:
```text
com.snowflake:spark-snowflake_2.12:2.12.0-spark_3.5
```
![alt text](image-8.png)
Once installed, the connector allows Databricks to communicate with Snowflake seamlessly via JDBC and Spark APIs.

## 🚀 After Installation

Restart your cluster (or detach + re-attach it).

Re-run your notebook cell:

df = spark.read.format("snowflake").options(**options).option("dbtable", "SALES").load()
df.show()


You should now see the Snowflake data appear successfully.
## 🧠 Step 4 — Attach Notebook

Once your compute (cluster) is running:
![alt text](image-5.png)
1. Navigate to **Workspace → Your Folder → Create → Notebook**  
2. Name it something like: `Snowflake_Connection`
3. Choose **Language: Python**
4. At the **top-left**, select your **running compute** from the dropdown menu to attach it.
5. Paste your **Lab 3 Snowflake connection code** into this notebook.

---
![alt text](image-6.png)

```python
options = {
    "sfURL": "YGVSEGU-JE02852.snowflakecomputing.com",
    "sfDatabase": "TRAINING_DB",
    "sfSchema": "RAW",
    "sfWarehouse": "COMPUTE_WH",
    "sfRole": "SYSADMIN",
    "sfUser": "geetha",
    "sfPassword": "MySnowflakecred1"
}
df = spark.read.format("snowflake").options(**options).option("dbtable", "SALES").load()
df.show()
```
![alt text](image-10.png)
## 💡 Summary

| **Step** | **Action** | **Where** |
|-----------|-------------|------------|
| 1 | Click **“Create compute”** | Top-right of Compute screen |
| 2 | Fill in Spark runtime, name, and size | Popup configuration form |
| 3 | Click **“Create compute”** | Bottom of the form |
| 4 | Wait for status **Running** | Compute list |
| 5 | Create **Notebook** and **Attach compute** | Workspace → Notebook |

---

## ✅ Next Steps
Once the compute is ready and your notebook is attached:
- Test your **Snowflake connection code** (Lab 3).
- Run sample queries to validate Spark–Snowflake integration.
- Monitor cluster logs from the **Compute tab** for debugging or performance review.
