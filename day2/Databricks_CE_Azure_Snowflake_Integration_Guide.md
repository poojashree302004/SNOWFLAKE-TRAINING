# ‚òÅÔ∏è Snowflake + Azure + Databricks Community Edition ‚Äì Step-by-Step Guide

> üéØ **Goal:**  
Set up Snowflake and Azure Data Lake, connect them with the *free Databricks Community Edition*, move data from Azure ‚Üí Databricks ‚Üí Snowflake, and query Snowflake using the Snowflake Connector and Snowpark API.

---

## üß© 1. Prerequisites

| Tool | Requirement | Notes |
|------|--------------|-------|
| **Snowflake** | Free 30-day trial | Choose Azure as cloud provider |
| **Databricks** | [Community Edition](https://community.cloud.databricks.com) | 100% free; hosted by Databricks |
| **Azure** | Free Subscription | Used for Azure Blob / Data Lake Storage |
| **Python 3.x** | Installed locally | Optional ‚Äì for Snowpark script testing |

---

## ‚öôÔ∏è 2. Step 1 ‚Äî Create & Configure Snowflake Trial

1. Visit: [https://signup.snowflake.com](https://signup.snowflake.com)
2. Choose:
   - **Cloud provider:** Microsoft Azure  
   - **Region:** your nearest Azure region (e.g. *East US 2*)
3. Log in to **Snowsight (UI)** ‚Üí You‚Äôll get a URL like:  
   `https://xy12345.east-us-2.azure.snowflakecomputing.com`
4. Note down:
   - `Account` (e.g., `xy12345`)
   - `Username`
   - `Password`
   - `Warehouse`: `COMPUTE_WH`
   - `Database`: `MY_PRACTICE_DB`
   - `Schema`: `MY_SCHEMA`

‚úÖ **Test your setup**
```sql
CREATE OR REPLACE TABLE SALES (
  ID INT, CUSTOMER STRING, REGION STRING, AMOUNT NUMBER(10,2)
);
INSERT INTO SALES VALUES (1, 'John', 'West', 5000);
SELECT * FROM SALES;
```

---

## üß± 3. Step 2 ‚Äî Set Up Azure Storage (ADLS / Blob)

1. In Azure Portal ‚Üí **Storage Accounts ‚Üí + Create**
2. Resource group ‚Üí choose any
3. Storage account name ‚Üí `storageaccountname`
4. Region ‚Üí same as Snowflake region  
5. After creation ‚Üí Go to **Access keys** and copy:
   - **Storage account name**
   - **Access key**
6. Under **Containers**, create:
   - Container name ‚Üí `salesdata`
   - Upload CSV file ‚Üí `sales_2025_10_13.csv`
![alt text](image.png)
‚úÖ Example CSV:
```csv
ID,CUSTOMER,REGION,AMOUNT
1,John,West,5000
2,Priya,East,3200
3,David,North,4100
4,Meena,South,2800
```
![alt text](image-1.png)
---

## üíª 4. Step 3 ‚Äî Launch Databricks Community Edition

1. Go to [https://community.cloud.databricks.com](https://community.cloud.databricks.com)
2. Sign in with any email.
3. Click **Compute ‚Üí Create Cluster**
   - Name: `CommunityCluster`
   - Runtime: `11.x (includes Apache Spark 3.x, Scala 2.12)`
4. Wait until it shows **Running (green dot)** ‚úÖ

---

## üîó 5. Step 4 ‚Äî Access Azure Blob Storage from Databricks CE

> ‚ö†Ô∏è *Community Edition doesn‚Äôt allow dbutils.fs.mount(), but you can access via wasbs://*

### Configure Access Key
```python
import pandas as pd

url = (
    "Azure storage container url with sas token"
)

df = pd.read_csv(url)
print(df.head())

```

![alt text](image-2.png)

‚úÖ You‚Äôll see your Azure file loaded into Databricks.

---

## ‚ùÑÔ∏è 6. Step 5 ‚Äî Connect Databricks to Snowflake

```python
%pip install snowflake-connector-python[pandas]

```

```python
 %restart_python
```
### Define connection options
```python
import snowflake.connector
from snowflake.connector.pandas_tools import write_pandas
import pandas as pd

url = (
    "Azure storage container url with sas token"
)

df = pd.read_csv(url, sep=",", encoding="utf-8-sig")
#df.columns = ["ID", "CUSTOMER", "REGION", "AMOUNT"]
#df["ID"] = df["ID"].astype(int)
#df["AMOUNT"] = df["AMOUNT"].astype(float)

conn = snowflake.connector.connect(
    user="snowflake_username",
    password="********",
    account="<snowflake.account>",
    warehouse="COMPUTE_WH",
    database="MY_PRACTICE_DB",
    schema="MY_SCHEMA"
)

success, nchunks, nrows, _ = write_pandas(conn, df, "SALES_FROM_AZURE")
print(f"‚úÖ Uploaded {nrows} rows successfully!")
conn.close()

```

![alt text](image-3.png)

## üë• 7. Step 6 ‚Äî Create Role and Privileges (Optional)

In Snowflake:
```sql
CREATE ROLE DATABRICKS_ROLE;
GRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE DATABRICKS_ROLE;
GRANT USAGE ON DATABASE TRAINING_DB TO ROLE DATABRICKS_ROLE;
GRANT USAGE ON SCHEMA PUBLIC TO ROLE DATABRICKS_ROLE;
GRANT SELECT, INSERT ON ALL TABLES IN SCHEMA PUBLIC TO ROLE DATABRICKS_ROLE;
GRANT ROLE DATABRICKS_ROLE TO USER TRAINER_USER;
```

‚úÖ Use this role in Databricks under `sfRole`.

---

## üè™ 8. Step 7 ‚Äî Explore Snowflake Marketplace

In Snowsight ‚Üí **Data ‚Üí Marketplace ‚Üí Search ‚ÄúCOVID-19‚Äù or ‚ÄúWeather Source‚Äù**  
‚Üí Click **Get Data ‚Üí Add to account**

Then query:
```sql
USE DATABASE COVID19_EPIDEMIOLOGICAL_DATA;
SELECT * FROM EPIDEMIOLOGY LIMIT 10;
```

---

## üß† 9. Step 8 ‚Äî Query Snowflake Using Snowpark

### Install Snowpark
```python
%pip install snowflake-snowpark-python
```

```python
%restart_python
```
### Initialize Session
```python
from snowflake.snowpark import Session
from snowflake.snowpark.functions import col

connection_params = {
    "user":"snowflake_username",
    "password":"********",
    "account":"<snowflake.account>",
    "warehouse":"COMPUTE_WH",
    "database":"MY_PRACTICE_DB",
    "schema":"MY_SCHEMA"
}

session = Session.builder.configs(connection_params).create()
```

### Query
```python
df_snow = session.table("SALES_FROM_AZURE")
df_snow.filter(col("AMOUNT") > 3000).show()
```

‚úÖ Output:
```
+----+---------+--------+--------+
| ID | CUSTOMER| REGION | AMOUNT |
+----+---------+--------+--------+
| 1  | John    | West   | 5000.0 |
| 2  | Priya   | East   | 3200.0 |
+----+---------+--------+--------+
```

---

## üìä 10. End-to-End Architecture
```
Azure Blob (ADLS)
    ‚Üì
Databricks Community Edition (Spark / Snowpark)
    ‚Üì
Snowflake (Warehouse + Schema + Table)
```

---

## ‚úÖ Outcome Summary

| Task | Status |
|------|--------|
| Create Snowflake Trial | ‚úÖ |
| Create Azure Blob Storage | ‚úÖ |
| Read Azure file in Databricks CE | ‚úÖ |
| Write data into Snowflake | ‚úÖ |
| Query in Snowflake | ‚úÖ |
| Run Snowpark Script | ‚úÖ |

---

## üßæ Trainer Notes
- Databricks CE runs on Databricks‚Äô infra (not Azure VMs), but connects fine to Azure and Snowflake.
- `dbutils.fs.mount` won‚Äôt work ‚Äî use direct `wasbs://` access.
- Use `ACCOUNTADMIN` role for learning to avoid permission issues.
- Drop tables post-lab to save Snowflake credits:
  ```sql
  DROP TABLE IF EXISTS SALES_FROM_AZURE;
  ```

---

**Author:** *Prepared by Snowflake & Databricks Specialist Trainer (GPT‚Äë5)*  
**Module:** Databricks CE + Azure + Snowflake Integration  
**Audience:** Beginners & Data Engineering Learners
