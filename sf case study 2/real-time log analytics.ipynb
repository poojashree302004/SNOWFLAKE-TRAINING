{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39efa99-58d9-4b5b-92ec-fcc0cbaf1a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FileInfo(path='abfss://logs-raw@sfhexastorage.dfs.core.windows.net/raw.json', name='raw.json', size=138, modificationTime=1760610959000)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(f\"abfss://logs-raw@{storage_account_name}.dfs.core.windows.net/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a3c440-e27f-42a6-a636-b9681bf17c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install Snowflake Connector and Snowpark packages\n",
    "%pip install snowflake-snowpark-python\n",
    "%pip install snowflake-connector-python\n",
    "%pip install snowflake-sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "826a34a6-1bfe-46fb-b67a-3fd3bf9086e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201e61a6-8755-4b6d-bb5d-f7b05540e714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Snowflake successfully!\n",
      "+------+--------------------+-----+\n",
      "| event|           timestamp| user|\n",
      "+------+--------------------+-----+\n",
      "| login|2025-10-16T09:23:00Z|pooja|\n",
      "|logout|2025-10-16T09:30:00Z| ravi|\n",
      "+------+--------------------+-----+\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "|\"event\"  |\"timestamp\"           |\"user\"  |\"EVENT_TIME\"         |\n",
      "-----------------------------------------------------------------\n",
      "|login    |2025-10-16T09:23:00Z  |pooja   |2025-10-16 09:23:00  |\n",
      "|logout   |2025-10-16T09:30:00Z  |ravi    |2025-10-16 09:30:00  |\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Data written to Snowflake table LOGS_CLEANED successfully!\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col, to_timestamp\n",
    "\n",
    "# -----------------------------\n",
    "# Snowflake Connection\n",
    "# -----------------------------\n",
    "sfOptions = {\n",
    "    \"account\": \"sqishot-fa68768\", \n",
    "    \"user\": \"poojashree\",\n",
    "    \"password\": \"Poojashree@307\",\n",
    "    \"warehouse\": \"COMPUTE_WH\",\n",
    "    \"database\": \"LOG_ANALYTICS\",\n",
    "    \"schema\": \"PUBLIC\",\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(sfOptions).create()\n",
    "print(\"Connected to Snowflake successfully!\")\n",
    "\n",
    "# -----------------------------\n",
    "# Connect to ADLS using Storage Account Key\n",
    "# -----------------------------\n",
    "storage_account_name = \"sfhexastorage\"\n",
    "storage_account_key = \"jBmM0OVBS0I8tvj1LLFb0qzJVIFk9+Qd/hNW0ZvNe6TiSbatHvHC6NoeSYm3nfbmy1hocQuVbnhw+AStRqeGIA==\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Read JSON logs from ADLS raw folder\n",
    "# -----------------------------\n",
    "logs_path = f\"abfss://logs-raw@{storage_account_name}.dfs.core.windows.net/raw.json\"\n",
    "\n",
    "logs_df = spark.read.json(logs_path)\n",
    "logs_df.show()  # Quick preview\n",
    "\n",
    "# Convert Spark DataFrame to Pandas (for small batch)\n",
    "logs_pd = logs_df.toPandas()\n",
    "\n",
    "# -----------------------------\n",
    "# Create Snowpark DataFrame from Pandas\n",
    "# -----------------------------\n",
    "sp_df = session.create_dataframe(logs_pd)\n",
    "\n",
    "# -----------------------------\n",
    "# Rename + Format Columns\n",
    "# -----------------------------\n",
    "sp_df_cleaned = (\n",
    "    sp_df\n",
    "    .with_column_renamed(\"user\", \"USER_NAME\")\n",
    "    .with_column_renamed(\"event\", \"EVENT\")\n",
    "    .with_column(\"EVENT_TIME\", to_timestamp(col('\"timestamp\"')))\n",
    ")\n",
    "\n",
    "sp_df_cleaned.show()\n",
    "'''\n",
    "# -----------------------------\n",
    "# Create database/schema/table if not exists\n",
    "# -----------------------------\n",
    "session.sql(\"CREATE DATABASE IF NOT EXISTS LOG_ANALYTICS\").collect()\n",
    "session.sql(\"USE DATABASE LOG_ANALYTICS\").collect()\n",
    "session.sql(\"CREATE SCHEMA IF NOT EXISTS PUBLIC\").collect()\n",
    "session.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE LOGS_CLEANED (\n",
    "        EVENT STRING,\n",
    "        USER_NAME STRING,\n",
    "        TIMESTAMP STRING,\n",
    "        EVENT_TIME TIMESTAMP_NTZ\n",
    "    )\n",
    "\"\"\").collect()\n",
    "'''\n",
    "# -----------------------------\n",
    "# Write cleaned data to Snowflake table\n",
    "# -----------------------------\n",
    "# Automatic insert, append mode\n",
    "sp_df_cleaned.write.mode(\"append\").option(\"column_quote\", '\"').save_as_table(\"LOGS_CLEANED\")\n",
    "print(\"Data written to Snowflake table LOGS_CLEANED successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "real-time log analytics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
